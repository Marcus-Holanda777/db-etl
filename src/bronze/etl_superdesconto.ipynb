{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755da940-2758-45a2-974f-c66689ecc90c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "IMPORTS"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql import DataFrame, Column\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cols_rename = [\n",
    "    \"filial\", \"cod_prod\", \n",
    "    \"periodo\", \"etiqueta\", \n",
    "    \"perc_dsc_cupom\", \"venda\", \n",
    "    \"venda_desconto\"\n",
    "]\n",
    "\n",
    "cols_cosmos = [\n",
    "    \"MVVC_CD_FILIAL_MOV\",\n",
    "    \"MVVP_NR_PRD\",\n",
    "    \"MVVC_DT_MOV\",\n",
    "    \"NUMERO_AUTORIZ_PAGUEMENOS\",\n",
    "    \"MVVP_PR_DSC_ITE\",\n",
    "    \"MVVP_VL_PRE_VDA\",\n",
    "    \"MVVP_VL_PRD_VEN\",\n",
    "]\n",
    "\n",
    "cols_pre_venda = [\n",
    "    \"VC_CD_FILIAL\",\n",
    "    \"VD_CD_PRODUTO\",\n",
    "    \"VC_DH_VENDA\",\n",
    "    \"VD_COD_ETIQUETA_ULCH\",\n",
    "    \"VD_PERC_DESCONTO\",\n",
    "    \"VD_VL_PRODUTO\",\n",
    "    \"VD_VL_PRODUTO_COM_DESCONTO\",\n",
    "]\n",
    "\n",
    "cols_autorizador = [\n",
    "    \"ulch_sq_autorizacao\",\n",
    "    \"ulch_preco_venda\",\n",
    "    \"ulch_percentual_desconto\",\n",
    "    \"ulch_fl_tipo_produto\",\n",
    "    \"ulch_cd_barras\",\n",
    "    \"ulch_fl_situacao\",\n",
    "    \"ulch_sq_produto\"\n",
    "]\n",
    "\n",
    "cols_produto = [\n",
    "    \"ulch_sq_produto\",\n",
    "    \"xxxx_dh_cad\",\n",
    "    \"ulch_lote\",\n",
    "    \"ulch_dt_vencimento\",\n",
    "    \"ulch_sq_produto\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d7c5cc-282a-49ad-ac6b-76c095aca721",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UTILS"
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(\n",
    "    catalog: str, \n",
    "    schema: str, \n",
    "    tablename: str\n",
    ") -> bool:\n",
    "    \n",
    "    return spark.catalog.tableExists(f\"{catalog}.{schema}.{tablename}\")\n",
    "\n",
    "\n",
    "def etiqueta(colname: str) -> Column:\n",
    "    return F.lpad(F.trim(colname), 30, \"0\").cast(T.StringType())\n",
    "\n",
    "\n",
    "def list_files(\n",
    "    path: str, \n",
    "    start: datetime, \n",
    "    end: datetime\n",
    "):\n",
    "    days = (end - start).days + 1\n",
    "    for day in range(days):\n",
    "        dt = start + timedelta(day)\n",
    "        yield f\"/Volumes/raw/super_desconto/{path}/{dt:%Y/%m/%d}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4368e7f3-53b7-4671-a4b9-87c8685b161f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VIEWS"
    }
   },
   "outputs": [],
   "source": [
    "def view_pre_venda(\n",
    "    path: str, \n",
    "    start: datetime, \n",
    "    end: datetime, \n",
    "    columns: list[str]\n",
    ") -> DataFrame:\n",
    "    \n",
    "    col_etiqueta = columns[3]\n",
    "    files_path = list(list_files(path, start, end))\n",
    "\n",
    "    return (\n",
    "        spark.read.parquet(*files_path)\n",
    "        .select(columns)\n",
    "        .withColumn(col_etiqueta, etiqueta(col_etiqueta))\n",
    "        .withColumnsRenamed(dict(zip(columns, cols_rename)))\n",
    "    )\n",
    "\n",
    "\n",
    "def view_cupom(start: datetime, end: datetime) -> DataFrame:\n",
    "    windows = (\n",
    "        Window.partitionBy(\"etiqueta\")\n",
    "         .orderBy(col(\"venda_desconto\").desc())\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        view_pre_venda(\"COSMOSMOV\", start, end, cols_cosmos)\n",
    "        .union(view_pre_venda(\"PRE_VENDA\", start, end, cols_pre_venda))\n",
    "        .withColumn(\"id\", F.row_number().over(windows))\n",
    "        .filter(col(\"id\") == 1)\n",
    "        .drop(\"id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def view_autorizador() -> DataFrame:\n",
    "    volume = \"/Volumes/raw/super_desconto/cosmos_v14b\"\n",
    "\n",
    "    file = \"cosmos_v14b_dbo_ultima_chance_autorizacao.parquet\"\n",
    "    file_hist = \"cosmos_v14b_dbo_ultima_chance_autorizacao_hist.parquet\"\n",
    "    \n",
    "    def inner_dataframe(filename: str, priority: int) -> DataFrame:\n",
    "        return (\n",
    "            spark.read.parquet(f\"{volume}/{filename}\")\n",
    "            .select(cols_autorizador)\n",
    "            .filter(col(\"ulch_fl_situacao\") == \"F\")\n",
    "            .withColumn(\"ulch_cd_barras\", etiqueta(\"ulch_cd_barras\"))\n",
    "            .withColumn(\"ulch_percentual_desconto\", F.coalesce(\"ulch_percentual_desconto\", F.lit(0)))\n",
    "            .withColumn(\"priority\", F.lit(priority))\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        inner_dataframe(file, 1)\n",
    "        .union(inner_dataframe(file_hist, 2))\n",
    "        .orderBy(\"priority\")\n",
    "        .dropDuplicates([\"ulch_cd_barras\"])\n",
    "        .drop(\"priority\")\n",
    "    )\n",
    "\n",
    "\n",
    "def view_produto() -> DataFrame:\n",
    "    volume = \"/Volumes/raw/super_desconto/cosmos_v14b\"\n",
    "    file = \"cosmos_v14b_dbo_ultima_chance_produto.parquet\"\n",
    "    file_hist = \"cosmos_v14b_dbo_ultima_chance_produto_hist.parquet\"\n",
    "    \n",
    "    def inner_dataframe(filename: str, priority: int) -> DataFrame:\n",
    "        return (\n",
    "            spark.read.parquet(f\"{volume}/{filename}\")\n",
    "            .select(cols_produto)\n",
    "            .withColumn(\"ulch_lote\", F.upper(F.trim(\"ulch_lote\")))\n",
    "            .withColumn(\"priority\", F.lit(priority))\n",
    "        )\n",
    "    \n",
    "    return (\n",
    "        inner_dataframe(file, 1)\n",
    "        .union(inner_dataframe(file_hist, 2))\n",
    "        .orderBy(\"priority\")\n",
    "        .dropDuplicates([\"ulch_sq_produto\"])\n",
    "        .drop(\"priority\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94dcff57-4ff4-40cc-a17f-a63383ef5ed5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AUXILIAR"
    }
   },
   "outputs": [],
   "source": [
    "# consulta full load\n",
    "autorizador = view_autorizador()\n",
    "produto = view_produto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839aafb1-e82a-4e27-bd9a-c00251647084",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EXECUTOR"
    }
   },
   "outputs": [],
   "source": [
    "# full table\n",
    "catalog = \"bronze\"\n",
    "schema = \"super_desconto\"\n",
    "table_name = \"venda\"\n",
    "\n",
    "cupom = view_cupom(\n",
    "    datetime(2022, 1, 1),\n",
    "    datetime(2022, 12, 31)\n",
    ")\n",
    "\n",
    "view_create = (\n",
    "    cupom\n",
    "     .join(autorizador, cupom.etiqueta == autorizador.ulch_cd_barras)\n",
    "     .join(produto, autorizador.ulch_sq_produto == produto.ulch_sq_produto)\n",
    "     .select(\n",
    "        autorizador.ulch_sq_autorizacao,\n",
    "        produto.ulch_sq_produto,\n",
    "        produto.xxxx_dh_cad,\n",
    "        cupom.periodo.alias(\"dt_venda\"),\n",
    "        cupom.filial,\n",
    "        cupom.cod_prod,\n",
    "        produto.ulch_lote,\n",
    "        produto.ulch_dt_vencimento,\n",
    "        cupom.etiqueta,\n",
    "        cupom.perc_dsc_cupom,\n",
    "        cupom.venda,\n",
    "        cupom.venda_desconto,\n",
    "        autorizador.ulch_preco_venda.alias(\"ulch_preco_venda\"), \n",
    "        autorizador.ulch_percentual_desconto, \n",
    "        autorizador.ulch_fl_tipo_produto\n",
    "     )\n",
    ")\n",
    "\n",
    "if not table_exists(catalog, schema, table_name):\n",
    "    print(f\"creating table {catalog}.{schema}.{table_name}\")\n",
    "\n",
    "    (\n",
    "        view_create.coalesce(1).write.mode(\"overwrite\")\n",
    "        .format(\"delta\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"updating table {catalog}.{schema}.{table_name}\")\n",
    "\n",
    "    target = DeltaTable.forName(spark, f\"{catalog}.{schema}.{table_name}\")\n",
    "    (\n",
    "        target.alias('t')\n",
    "        .merge(view_create.alias('s'), \"t.etiqueta = s.etiqueta\")\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ce82df-a0e0-4a5c-8229-7d4f0425b179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select \n",
    " date_trunc('MONTH', dt_venda) as periodo, \n",
    " count(*)                      as registros,\n",
    " sum(venda_desconto)           as valor\n",
    "from bronze.super_desconto.venda\n",
    "group by all\n",
    "order by 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c109bfd9-5702-4819-84e2-4c76fc58416e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "EXPORT ATHENA"
    }
   },
   "outputs": [],
   "source": [
    "from athena_mvsh import Athena, CursorParquetDuckdb\n",
    "\n",
    "cursor = CursorParquetDuckdb(\n",
    "    s3_staging_dir=\"#\",\n",
    "    result_reuse_enable=True,\n",
    "    aws_access_key_id=\"#\",\n",
    "    aws_secret_access_key=\"#\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "table_athena = \"super_desconto_vendas\"\n",
    "periodos = [(ano, mes) for ano in range(2022, 2025) for mes in range(1, 13)][1:-2]\n",
    "\n",
    "for (ano, mes) in periodos:\n",
    "    print(f\"periodo: {ano}-{mes}\")\n",
    "\n",
    "    df = (\n",
    "        spark.table(\"bronze.super_desconto.venda\")\n",
    "        .filter((F.year(\"dt_venda\") == ano)  & (F.month(\"dt_venda\") == mes))\n",
    "    )\n",
    "\n",
    "    columns = {c.name:col(c.name).cast(T.DoubleType()) for c in df.schema if isinstance(c.dataType, T.DecimalType)}\n",
    "    columns.update({\"ulch_dt_vencimento\": col(\"ulch_dt_vencimento\").cast(T.TimestampNTZType())})\n",
    "    df = df.withColumns(columns)\n",
    "    df = df.toPandas()\n",
    "\n",
    "    with Athena(cursor=cursor) as cliente:\n",
    "        cliente.write_table_iceberg(\n",
    "            data=df,\n",
    "            table_name=table_athena,\n",
    "            schema=\"prevencao-perdas\",\n",
    "            location=f\"#/tables/{table_athena}/\",\n",
    "            if_exists='append'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79d955c-3d0d-4372-92fc-7adea11bd7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783ffe4-6be5-4e20-88b7-ff301eb17c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5772100970431055,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_superdesconto",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
